{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0edf675",
   "metadata": {},
   "source": [
    "## M5 Forecasting Competition - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4824d2",
   "metadata": {},
   "source": [
    "### Objectives:\n",
    "> #### To provide summary information about the M5 Forecasting Competition.\n",
    "> #### To provide summary findings from the M5 Forecasting Accuracy Challenge\n",
    "### Outline:\n",
    "> #### 1) What is the M5 Competition?\n",
    "> #### 2) Key Differences vs the previous four\n",
    "> #### 3) Competition Timeline\n",
    "> #### 4) Dataset\n",
    "> #### 5) Accuracy Challenge\n",
    ">> #### 5-a) Performance Measure\n",
    ">> #### 5-b) Performance Benchmark\n",
    ">> #### 5-c) Prizes\n",
    ">> #### 5-d) Participation\n",
    ">> #### 5-e) Results\n",
    ">> #### 5-f) Top Five (5) Winners\n",
    ">> #### 5-g) Seven (7) Key Findings\n",
    ">> #### 5-h) Conclusion\n",
    "\n",
    "Prepared by  Leo R. De Velez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc79f46",
   "metadata": {},
   "source": [
    "### 1) What is the M5 Competition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42034d",
   "metadata": {},
   "source": [
    "The M5 Competition was the 5th in the series of forecasting competitions that was named after its leader and main proponent, Spyros Makridakis.  The first contest of the series was held in 1982.\n",
    "<br><br>\n",
    "a. The <b>M Competition</b> in 1982 processed 1001 non-real-time series where 15 forecasting methods, with 9 variations, were tested.\n",
    "<br><br>\n",
    "b. The <b>M2 Competition</b> in 1993 worked with 23 real-time series and tested 16 forecasting methods plus two combination forecasts and one overall average forecast.\n",
    "<br><br>\n",
    "c. The <b>M3 Competition</b> in 2000 dealt with 3003 time series and tested 24 forecasting methods.\n",
    "<br><br>\n",
    "d. The <b>M4 Competition</b> in 2020 analyzed 100,000 time series and tested the applicability and accuracy of the major statistical and Machine Learning methods. The participant from UBER won the competition.\n",
    "<br><br>\n",
    "e. The <b>M5 Competition</b> in 2021 studied 42,000+ hierarchical timeseries from Walmart and tested also the major statistical and Machine Learning methods, including Deep Learning methods.  The contest had two challenges, the Forecasting Accuracy and the Uncertainty Challenge.  The total Prize money was USD100K.\n",
    "<br><br>\n",
    "f. There is an on-going <b>M6 Competition</b> where participants have to forecast the future prices of 50 Stocks and 50 Excange Traded Funds (EFT).  The participants are also being asked rank these Stocks and EFTs for investment.  The M6 Competition total Prize money is USD300K.\n",
    "<br>\n",
    "<br>\n",
    "The objective of the M5 Competition was <i>\"to advance the theory and practice of forecasting by identifying the method(s) that provide the most accurate point forecasts for each of the 42,840 time-series of the competition.\"</i> (Makridakis Competitions) \n",
    "<br><br>\n",
    "Another objective of the M5 competition was \"to elicit information to estimate the uncertainty distribution of the realized values of these series as precisely as possible.\" (Makridakis Competitions) \n",
    "<br>\n",
    "<br>The M5 Competition had two parallel Kaggle competitions that used the same Walmart dataset. <i>\"The first (M5 Forecasting Competition – Accuracy) requiring 28 days ahead point forecasts and the second (M5 Forecasting Competition – Uncertainty) 28 days ahead probabilistic forecasts for the corresponding median and four prediction intervals (50%, 67%, 95%, and 99%).”</i> (Makridakis Competitions) \n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br><i>Makridakis Competition</i>. Wikipedia. Accessed 2022 Nov 2.  https://en.wikipedia.org/wiki/Makridakis_Competitions\n",
    "<br><i>M6 Forecasting Competition</i>. MOFC Website. https://mofc.unic.ac.cy/  \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a138f2",
   "metadata": {},
   "source": [
    "### 2) Key Differences of M5 vs the previous four M's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e86b9",
   "metadata": {},
   "source": [
    "There were five key differences between M5 Competition and the first four M-Competitions.\n",
    "<br>\n",
    "<br><b>1) DATA SET</b>\n",
    "<br>\n",
    "<ul>The M5 Competition dataset came from Walmart.  The dataset was composed of daily sales informationo for each product item, at each store location.  The product sales were grouped or aggregated into the three geographical areas, into the ten store stores and the corresponding store-category and store-department levels.\n",
    "</ul>\n",
    "<br><b>2) EXOGENOUS VARIABLES</b>\n",
    "<br>\n",
    "<ul>The M5 Competition provided information on exogenous variables that can affect the sales performance of each product, department, category, stores, or geograpical location.  These information can help in improving the accuracy of the forecast.  These information includes sell prices, promotions, days of the week, and special events like major sport events (ex. Super Bowl), special days (ex. Valentine’s Day) and holidays (ex. Orthodox Easter).\n",
    "</ul>\n",
    "<br><b>3) DISTRIBUTION UNCERTAINTY</b>\n",
    "<br>\n",
    "<ul>The M5 Competition included additional task for the participants to provide uncertainty distribution of their forecast.</ul>\n",
    "<br><b>4) TWO-TRACK COMPETITION</b>\n",
    "<br>\n",
    "<ul>The M5 Competition had two-tracks competition that used the same data set.  The first was a forecast accuracy competiton and the second was a forecast uncertianty distribution competition.  For the forecast accuracy, the participants were asked to provide 28 days forecast. For the uncertainty distribution, the participants were asked to provide 28 days ahead of probabilistic forecast for the median and for 50%, 67%, 95% and 99% predication intervals.</ul>\n",
    "<br><b>5) INTERMITTENCY OF DATA</b>\n",
    "<br>\n",
    "<ul>The M5 Competition, for the first time, used time series with intermittent data due to sporading customer demands, lack of demand, or lack of supply.</ul>\n",
    "<br>For more detailed information, please access the M5 Competition Competitors' Guide at https://mofc.unic.ac.cy/wp-content/uploads/2020/03/M5-Competitors-Guide-Final-10-March-2020.docx\n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br>\n",
    "<i>THE M5 COMPETITION - Competitors’ Guide</i>. Downloaded from  https://mofc.unic.ac.cy/wp-content/uploads/2020/03/M5-Competitors-Guide-Final-10-March-2020.docx \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a71c27",
   "metadata": {},
   "source": [
    "### 3) Competition Timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2c1da",
   "metadata": {},
   "source": [
    "<b>START AND COMPLETION DATE</b>\n",
    "<ul>The M5 competition started on March 2, 2020.  The competition was completed on June 30, 2020.\n",
    "</ul>\n",
    "<b>VALIDATION PHASE</b>\n",
    "<ul>To support the participants in testing their forecasting models, a validation window was given from March 2, 2020 to May 31, 2020.\n",
    "</ul>\n",
    "<b>TUNING / TESTING PHASE</b>\n",
    "<ul>The tuning and testing window was from June 1, 2020 until June 20, 2020.\n",
    "</ul>\n",
    "<b>FORECAST SUBMISSION</b>\n",
    "<ul>The forecast submission window was from the start of the competition on March 2, 2020 up to June 20, 2020.  Participants can submit multiple forecasts for validation but had to choose one of their forecast as final forecast for submission.\n",
    "</ul>\n",
    "<br>\n",
    "References:\n",
    "<br>\n",
    "<i>THE M5 COMPETITION - Competitors’ Guide</i>. Downloaded from  https://mofc.unic.ac.cy/wp-content/uploads/2020/03/M5-Competitors-Guide-Final-10-March-2020.docx \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbca710",
   "metadata": {},
   "source": [
    "### 4) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c965f9",
   "metadata": {},
   "source": [
    "The M5 Competition used the dataset provided by Walmart that consist of daily sales informationo for each product item, at each store location. The product sales were grouped or aggregated into the three geographical areas (California, Texas, Wisconsin), into the ten store stores (4 in CA, 3 in TX, and 3 in WI) and the corresponding store-category and store-department levels.\n",
    "<br><br>\n",
    "The M5 Competition provided information regarding external variables such as special events, holidays, selling prices, and SNAP activities.\n",
    "<br><br>\n",
    "The required files can be downloaded from the M5 Forecasting Competition website (https://www.kaggle.com/competitions/m5-forecasting-accuracy/data). These were:\n",
    "<br>\n",
    "<br>\n",
    "<b>CALENDAR INFORMATION</b>\n",
    "<ul>\n",
    "Filename: calendar.csv - The calendar file contains the dates and products sold on each date.\n",
    "</ul>\n",
    "</ul>\n",
    "<b>SELL PRICE DATASET</b>\n",
    "<ul>Filename: sell_prices.csv - The sell price file contains the price and sales volume of each products sold at each store each day.\n",
    "</ul>\n",
    "<b>VALIDATION DATASET</b>\n",
    "<ul>\n",
    "Filename: sales_train_validation.csv - The validation file contains the unit sales data per day per product per store from Day 1 to Day 1913.\n",
    "</ul>\n",
    "<b>EVALUATION DATASET</b>\n",
    "<ul>Filename: sales_train_evaluation.csv - The evaluation file contains the unit sales data per day per product per store from Day 1 to Day 1941.\n",
    "</ul>\n",
    "</ul>\n",
    "<b>SAMPLE SUBMISSION FORM</b>\n",
    "<ul>Filename: sample_submission.csv - The sample submission form provided the correct format for forecast files.\n",
    "</ul>\n",
    "<br>\n",
    "References:\n",
    "<br>\n",
    "<i>M5 presentation in Progress</i>. 40th International Symposium on Forecasting. October 28th 2020. Page 14. Downloaded on Nov. 2, 2022. https://www.researchgate.net/profile/Evangelos-Spiliotis/publication/344947668_The_M5_Competition_in_Progress/links/5fa3fb1a92851cc286960b0c/The-M5-Competition-in-Progress.pdf \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f68e6",
   "metadata": {},
   "source": [
    "### 5) Accuracy Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee21dda",
   "metadata": {},
   "source": [
    "There were two parallel challenges for the M5 Competition.  These were Forecasting Accuracy and Uncertainty Distribution challenges.  The two challenges are different from each other in terms of the following:\n",
    "<br><b>\n",
    "    <ul><b>1) SUBMISSION TEMPLATE,</b></ul>\n",
    "    <ul><b>2) PERFORMANCE MEASURE,</b></ul> \n",
    "    <ul><b>3) PRIZES, and<b></ul> \n",
    "    <ul><b>4) BENCHMARKS</b></ul>\n",
    "<br></b>\n",
    "The template required <i>“submitting forecasts that corresponded to only 30,490 series of the lowest cross-sectional aggregation level of the data set (level 12) rather than all 42,840 series in the competition because the M5 series are hierarchically structured, and thus we expected the corresponding forecasts to be coherent (forecasts at the lower levels have to sum up to the forecasts at the higher levels so that the forecasts across different levels are aligned)\"</i> (Makridakis et al., 2022).\n",
    "<br><br>\n",
    "<i>Thus, it was assumed that the forecasting approaches used by the contestants for predicting all 42,840 series in the competition would result in coherent forecasts, so the forecasts at the lowest aggregation level could be appropriately aggregated (summed up) to automatically compute those at the remaining levels\"</i> (Makridakis et al., 2022).\n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br>\n",
    "Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 )\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416a257",
   "metadata": {},
   "source": [
    "### 5-a) Performance Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30348b43",
   "metadata": {},
   "source": [
    "Based on the learnings from previous M Competitions, M5 used a modified Mean Absolute Scaled Error (MASE) that was originally proposed by  by Hydman and Koeler as another way to measure forecasting accuracy.\n",
    "<br><br>\n",
    "<i>“The M5 “Accuracy” competition utilized a variant of the MASE originally proposed by Hyndman and Koehler (2006) called the root mean squared scaled error (RMSSE), which is defined as follows:</i>\n",
    "<br>\n",
    "\n",
    "    \n",
    "<ul><h1>RMSSE = $\\sqrt{\\frac{\\frac{1}{h}\\sum_{t=n+1}^{n+h} ( y_{t} - \\hat{y}_{t} )^2}{\\frac{1}{n-1}\\sum_{t=2}^{n} (y_{t} - \\hat{y}_{t-1} )^2}}    .$</h1></ul>\n",
    "\n",
    "<i><br>where <b> $y_{t}$ </b> is the actual future value of the examined time series at point <b> t, $\\hat{y}_{t}$ </b> is the forecast by the method under evaluation, <b> n </b> is the length of the training sample (number of historical observations), and <b> h </b> is the forecasting horizon (28 days). It should be noted that the denominator of RMSSE (in-sample, one-step-ahead mean squared error of the naive method) is computed only for the periods during which the examined products are actively sold, i.e., the periods following the first non-zero demand observed for the series under evaluation, because many of the products included in the data set started to be sold later than the first available date\"</i> (Makridakis et al., 2022).\n",
    "<br>\n",
    "<br>\n",
    "<i>“Similar to MASE, RMSSE is independent of the data scale and it has predictable behavior, i.e., it becomes infinite or undefined only when all the errors with the Naive method are equal to zero, as well as having a defined mean and finite variance, and it is symmetric in the sense that it penalizes equally positive and negative forecast errors. The choice of this particular measure can also be justified as follows.\n",
    "<br>\n",
    "Many of the series in the competition are characterized by intermittency, involving sporadic unit sales with many zeros. Thus, absolute errors that are optimized for the median (Schwertman et al., 1990) would assign lower scores (better accuracy) to forecasting methods that derive forecasts close to zero. However, the objective of the competition is to accurately forecast the average sales. Thus, the accuracy measure employed is built on squared errors that are optimized for the mean (Kolassa, 2016). In contrast to other measures with similar statistical properties, such as relative errors and measures (Davydenko & Fildes, 2013), RMSSE can be safely computed for all M5 series because it does not rely on divisions with values that could be equal or close to zero. For example, this is typically the case with percentage errors when  is equal to zero or relative errors when the error of the benchmark used for scaling is zero”</i> (Makridakis et al., 2022).\n",
    "<br>\n",
    "<br>\n",
    "<i>“After estimating RMSSE for all 42,840 time-series in the competition (the average accuracy reported for each series across the complete forecasting horizon), the overall accuracy of the forecasting method was computed by averaging the RMSSE scores across all series in the data set using appropriate weights. This measure, called the weighted RMSSE (WRMSSE), is defined as follows:</i>\n",
    "<br>\n",
    "<br>\n",
    "<ul>\n",
    "    <h2>WRMSSE = $ \\sum_{i=1}^{42,840} w_{i} x RMSSE_{i} $ </h2>\n",
    "    \n",
    "</ul>\n",
    "<br>\n",
    "<i>where <b>$w_{i}$</b> is the weight and <b>$RMSSE_{i}$</b> is the score of the ith series in the competition. Lower WRMSSE scores indicate more accurate forecasts. It should be noted that the estimation of WRMSSE differs from the approaches applied in previous M competitions. In the first three competitions, all of the errors were computed both per series and per forecasting horizon, and then equally averaged together. In M4, the errors were first averaged per series in exactly the same manner as M5, but then averaged again using equal weights.\n",
    "<br><br>The weights for the WRMSSE measure were computed based on the last 28 observations of the final training sample in the data set, specifically the cumulative actual dollar sales in each series in that particular period (sum of units sold multiplied by their respective price)”</i>  (Makridakis et al., 2022).\n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br>\n",
    "Hyndman, R., Koehler, A. (2005). <i>Another look at measures of forecasting accuracy</i>. November 2, 2005.  Page 12.Downloaded from https://robjhyndman.com/papers/mase.pdf\n",
    "<br>\n",
    "Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions</i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 )\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac133726",
   "metadata": {},
   "source": [
    "### 5-b) Performance Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59f3642",
   "metadata": {},
   "source": [
    "The Accuracy Challenge competition used 24 Forecasting Methods as Benchmarks. These benchmarks included both the statistical and Machine Learning methods and the combination of these two.\n",
    "<br><br>\n",
    "Of these 24 benchmarking methods, the three forecasting methods that were cited in the competition results' report are the Naive, the sNaive and the Exponential Smoothing with Bottom-Up reconcilation.\n",
    "<br>\n",
    "<ul>1. <b>Naive Method (Naive)</b>: \n",
    "    <br><br>\n",
    "    <ul><h3>$\\hat{y}_{t} = y_{t-1} .$</h3></ul>\n",
    "</ul>\n",
    "<br>\n",
    "<ul>2. <b>Seasonal Naive Method (sNaive)</b>: \n",
    "    <br><br>\n",
    "    <ul><h3>$\\hat{y}_{t} = y_{t-m} ,$</h3>\n",
    "    <br>where <b>m = 7</b> \n",
    "    </ul>\n",
    "</ul>\n",
    "<br>\n",
    "<ul>\n",
    "    3. <b>Exponential Smoothing with bottom-up reconciliation (ES bu)</b>:\n",
    "</ul    \n",
    "    <br><br>\n",
    "    <ul><ul><h3>$\\hat{y}_{t} = ay_{t} + (1-a)\\hat{y}_{t-m} .$</h3>\n",
    "        <br>where <b>m = 7</b> \n",
    "        <br>and <b>a</b> is typically from 0.1 to 0.3\n",
    "    </ul></ul>    \n",
    "\n",
    "<br>\n",
    "References:\n",
    "<br>Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions - Appendix: Description and performance of benchmarks.<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://ars.els-cdn.com/content/image/1-s2.0-S0169207021001874-mmc1.pdf\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c287e3",
   "metadata": {},
   "source": [
    "### 5-c) Prizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029f38f",
   "metadata": {},
   "source": [
    "The M5 Competition had a total prize money of USD 100,000.  It was divided equally to the two parallel competitions; the Forecasting Accuracy and Uncertainty Distribution.\n",
    "<br>\n",
    "For the M5 Forecasting Accuracy Competition, the USD 50,000 prize money were awarded  follows:\n",
    "<br>\n",
    "<ul><b>First Prize - USD25,000</b></ul>\n",
    "<ul><b>Second Prize - USD10,000</b></ul>\n",
    "<ul><b>Third Prize - USD5,000</b></ul>\n",
    "<ul><b>Fouth Prize - USD3,000</b></ul>\n",
    "<ul><b>Fifth Prize - USD2,000</b></ul>\n",
    "<ul><b>Student Prize - USD5,000</b></ul>\n",
    "There was no restriction for winning both the Regular 1st to 5th prize and the student prize. There was also no restriction for winning both the Accuracy and the Uncertainty Competitions.\n",
    "<br><br>\n",
    "The prize money came from the following sponsors:\n",
    "<br>\n",
    "<ul><b>Kaggle = USD40,000</b></ul>\n",
    "<ul><b>Google = USD20,000</b></ul>\n",
    "<ul><b>MOFC   = USD40,000</b></ul>\n",
    "<ul><b>Walmart = USD10,000</b></ul>\n",
    "<ul><b>Uber = USD5,000</b></ul>\n",
    "<ul><b>IIF = USD5,000</b></ul>\n",
    "<br>\n",
    "References:\n",
    "Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 )\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11b671",
   "metadata": {},
   "source": [
    "### 5- d) Participation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fccb6a",
   "metadata": {},
   "source": [
    "<b>SUMBISSIONS = 88,136\n",
    "<br>\n",
    "<br>PARTICIPANTS = 7,092\n",
    "<br>\n",
    "<br>TEAMS = 5,507 Teams\n",
    "<br>\n",
    "<br>COUNTRIES = 101 Countries\n",
    "<br>\n",
    "<br>FIRST TIME PARTICIPANTS = 1,563\n",
    "<br>\n",
    "<br>FIRST TIME PARTICIPANTS IN TOP 100 = 15\n",
    "</b>\n",
    "<br>\n",
    "<br>\n",
    "The M5 Forecasting Accuracy Competition received a total of 88,136 forecast submissions from 7,092 participants, who formed 5,507 different teams and who came from 101 different countries.\n",
    "<br>\n",
    "<br>There were 1563 participants who joined the Kaggle competition for the first time.  Among these first-timers, 15 landed into the top 100.\n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br>Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 )\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de847ae6",
   "metadata": {},
   "source": [
    "### 5-e) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e53e7",
   "metadata": {},
   "source": [
    "<b>SUBMITTED FORECAST VS. BENCHMARKS</b>\n",
    "<br>\n",
    "<br>\n",
    "Below is the summary of Participants' Peformance Based on Forecast that they Selected for Final Evaluation (not necessarily their Best Forecast).\n",
    "<ul><b>2666 submissions (48.4%) -- Better than Naive Benchmark</b></ul>\n",
    "<ul><b>1972 submission (35.8%) -- Better than Seasonal Naive Benchmark</b></ul>\n",
    "<ul><b>415 submission (7.5%) -- Better than Exponential Smoothing (w/ bottom up reconcillation)</b></ul>\n",
    "<br>\n",
    "<b>BEST FORECAST VS. BENCHMARK</b>\n",
    "<br>\n",
    "<br>\n",
    "Below is the corresponding summary of Participants' Peformance Based on their Best Forecast (not necessarily submitted as final forecast for Evaluation).\n",
    "<ul><b>3510 submissions (63.7%) -- Better than Naive Benchmark</b></ul>\n",
    "<ul><b>2685 submission (48.8%) -- Better than Seasonal Naive Benchmark</b></ul>\n",
    "<ul><b>672 submission (12.2%) -- Better than Exponential Smoothing (w/ bottom up reconcillation)</b></ul>\n",
    "Many submitted their less accurate forecast for final evaluation.  They may have been misled by their highest validation scores from their different forecasts.\n",
    "<br>\n",
    "<br>\n",
    "<b>MAJORITY WERE OUTPERFORMED BY TOP BENCHMARK</b>\n",
    "<br>\n",
    "<br>\n",
    "The exponential smoothing with bottom up reconcillation forecasting method was the top benchmark.  It performed better against most of the submitted forecasts.  It performed 13% better accuracy than the majority of the submissions.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>TOP PERFORMERS VS. TOP BENCHMARK</b>\n",
    "<br>\n",
    "<br>There were 415 teams who submitted forecast with performance above the top benchmark.  Of these 415 teams that managed to outperform the top benchmarks in the competition, \n",
    "<ul><b>1) 5 teams performed more than 20% above</b> the top benchmark</ul>\n",
    "<ul><b>2) 42 teams peforformed more than 15% above</b> the top benchmark</ul>\n",
    "<ul><b>3) 106 teams performed more than 10% above</b> the top benchmark</ul>\n",
    "<ul><b>4) 249 teams performed more than 5% above</b> the top benchmark</ul>\n",
    "<br>\n",
    "The substantial improvements in the forecasting accuracy of the top performers against the top benchmark demonstrated the superior peformance of the forecasting methods that were used.\n",
    "<br>\n",
    "<br>The top five performers had forecasting accuracy improvement of more than 20% compared with the top benchmark.  It was a clear win for these top five performers.\n",
    "<br>\n",
    "<br>\n",
    "<b>TOP BENCHMARK</b>\n",
    "<br>\n",
    "<br>\n",
    "Being relatively old, simple, easy, and cheap to use, the exponential smoothing forecasting method did remarkable well.  It out-performed 92.5% of the participating teams.\n",
    "<br>\n",
    "<br>\n",
    "<b>MACHINE LEARNING PEFORMANCE</b>\n",
    "<br>\n",
    "<br>\n",
    "The top five winning team utilized a more sophisticated Machine Learning methods and they managed to out-perform the top benchmark by significant and convincing amount.\n",
    "<br>\n",
    "<br>\n",
    "However, it is still a challenge to generalize that machine learning will perform better than statistical method such as exponential smoothing.\n",
    "<br>\n",
    "<br>\n",
    "The challenges are on the need more time and skills for the users of Machine Learning methods to make and tune the models.  There is plug and play model of Machine Learning.  In contrast, the statistical models such as the exponential smoothing, are mostly ready to use and does not need much experience and expertise to be effective in using it. \n",
    "<br>\n",
    "<br>\n",
    "<b>CONCLUSION</b>\n",
    "<br>\n",
    "<br>    \n",
    "<i>\"Therefore, it is important to consider whether it is necessary for retailers to focus on new, innovative methods without ensuring that their accuracy improvements are substantial compared with simple methods given the extra expenses incurred by skillful data scientists and powerful infrastructure.\n",
    "<br><br>Clearly, this finding may have been biased to some extent given that many of the teams that failed to outperform ES_bu could have lost interest in the competition or decided to have no further involvement in M5, thereby not improving their initial submissions. \n",
    "<br><br>Nevertheless, this did not influence the fact that the standard time series forecasting methods were competitive and difficult benchmarks to beat\"</i> (Makridakis et al., 2022)​.\n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br>\n",
    "Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 )\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32386c",
   "metadata": {},
   "source": [
    "### 5-f) Top Five (5) Winners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1031141",
   "metadata": {},
   "source": [
    "<b>1st Place - YJ_STU (YeonJun In)</b> \n",
    "<br><br>\n",
    "Used the LightGBM model.  The team created 220 LightGBM models and trained these models to forecast product per store using the per store, store-category, and store-department time series data.  They used cross-validation approach to fine-tune the models.  They used the mean and standard deviation of errors in the fine tuning process. Then, they did both the recursive and non-recursive forecasting for each level (store-product, store-category, and store-department) and combined the forecast outputs together at equal weights.\n",
    "<br><br>\n",
    "<b>2nd Place - Matthias (Matthias Anderer)</b> \n",
    "<br><br>\n",
    "Used the LightGBM model.  Similar to the first place winner, the team combined the results from the various LightGMB models at equal weights. They trained the LightGBM model on per store level and used five different external multipliers.  The external multipiers were based on the aggregated forecasts from the N-BEATS model, a type of Deep Neural model.\n",
    "<br>\n",
    "<br>\n",
    "<b>3rd Place - mf (Yunho Jeon & Sihyeon Seong)</b>\n",
    "<br><br>\n",
    "Use Deep Learning Neural Network model.  The team combined the results from their 43 Recursive Deep Learning Neural Network at equal weights.\n",
    "<br><br>\n",
    "<b>4th Place - monsaraida (Masanori Miyahara)</b>\n",
    "<br>\n",
    "<br>\n",
    "Used the LightGBM model.  The team used a Non-Recursive LightGBM model.  They made four models for each of the ten store. Each of the four models was used to forecast one of the four-week horizon.\n",
    "<br><br>\n",
    "<b>5th Place - Alan Lahoud (Alan Lahoud)</b>\n",
    "<br>\n",
    "<br>\n",
    "Used the LightGBM model.  The team used a Recursive LightGBM model.  They made 78 models and  fine tuned them through external adjustments to achieve the same store-department mean forecast as mean of the previous month. \n",
    "<br>\n",
    "<br>\n",
    "References:\n",
    "<br><i>M5 presentation in Progress</i>th International Symposium on Forecasting. October 28th 2020. Page 14. Downloaded on Nov. 2, 2022. https://www.researchgate.net/profile/Evangelos-Spiliotis/publication/344947668_The_M5_Competition_in_Progress/links/5fa3fb1a92851cc286960b0c/The-M5-Competition-in-Progress.pdf \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c41df",
   "metadata": {},
   "source": [
    "### 5-g) Seven (7) Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d25fe",
   "metadata": {},
   "source": [
    "<b>Finding No.1</b>\n",
    "<br>\n",
    "<ul>The Machine Learning forecasting method demostrated superior performance in forecasting accuracy against the top statistical benchmark such as the exponential smoothing.\n",
    "</ul>\n",
    "<br>\n",
    "<b>Finding No.2</b> \n",
    "<br>\n",
    "<ul>Improvement in forecasting accuracy performance from combining forecast from different variations of the main model.\n",
    "</ul>\n",
    "<br>\n",
    "<b>Finding No.3</b>\n",
    "<br>\n",
    "<ul>Improvement in forecasting accuracy performance from cross-learning from different data series.\n",
    "</ul>\n",
    "<br>\n",
    "<b>Finding No.4</b>\n",
    "<br>\n",
    "<ul>Significant improvement of more than 20% in forecasting accuracy by the winning methods compared to the statistical benchmark in terms of average WRMSSE.\n",
    "</ul>\n",
    "<br>\n",
    "<b>Finding No.5</b>\n",
    "<br>\n",
    "<ul>Improvement in forecasting accuracy peformance from external adjustments based on information about external factors.  \n",
    "</ul>\n",
    "<br>\n",
    "<b>Finding No.6</b>\n",
    "<br>\n",
    "<ul>The value of Cross Validation when using complex forecasting methods.  The value added comes from being able to objectively measure the forecast accuracyu.  Another value added comes from avoiding overfitting. It also help in reducing the forecasting uncertainty band.\n",
    "</ul>\n",
    "<br>\n",
    "<b>Finding No.7</b>\n",
    "<br>\n",
    "<ul>The forecasting accuracy of some methods have improved when the given explanatory variables, such as holidays, sales, and promotions, were included in the training and tuning process. \n",
    "</ul>\n",
    "<br>\n",
    "References:\n",
    "<br>Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 )\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85cf78",
   "metadata": {},
   "source": [
    "### 5h) Accuracy Challenge: Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a2ff75",
   "metadata": {},
   "source": [
    "There were three new findings from the M5 Forecasting Accuracy Competition.  These were:\n",
    "<br>\n",
    "<br>\n",
    "<b>1) Superior Accuracy of LightGBM</b>\n",
    "<br>\n",
    "<ul>The superior accuracy of the LightGBM method for predicting hierarchical retail sales resulted in substantial improvements compared with the benchmarks considered. \n",
    "</ul>\n",
    "<br><b>2) Improved Baseline Model Accuracy with External Adjustments</b>\n",
    "<br>\n",
    "<ul>The external adjustments utilized in some methods were beneficial for improving the accuracy of the baseline forecasting models.\n",
    "</ul>\n",
    "<br><b>3) Use of Exogenous Variables to Improve Forecasting Accuracy</b>\n",
    "<ul>\n",
    "Exogenous/explanatory variables were important for improving the forecasting accuracy of time series methods.\n",
    "</ul>\n",
    "<br>\n",
    "<br>\n",
    "There were also three previous findings from the previous M Competitions that were reaffirmed by the M5 Forecasting Accuracy Competition.  These were:\n",
    "<br>\n",
    "<br>\n",
    "<b>1) Combining</b>\n",
    "<br>\n",
    "<br>\n",
    "<b>2) Cross-learning</b>\n",
    "<br>\n",
    "<br>\n",
    "<b>3) Cross-validation</b>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<b>Shift to Machine Learning Methods</b>\n",
    "<br>\n",
    "<br>\n",
    "<i>\"The exceptional performance of statistical methods versus ML method found by Makridakis et al. (2018b), as well as in the early Kaggle competitions (Bojer & Meldgaard, 2021), first shifted toward both ML and statistical methods in the M4 competition, and then to exclusively ML methods in the Kaggle competitions after 2018 and M5, as described in this study. It will very interesting to see whether ML methods continue to dominate statistical methods in the future, particularly for other types of data that are not exclusively related to hierarchical retail sales applications.\n",
    "<br><br>\n",
    "Finally, the integration of statistics and data science into a unique field covering all academic aspects of forecasting and uncertainty is important, as well as determining how to increase the usage of forecasting in organizations by persuading executives of the benefits of systematic forecasting for improving their bottom line (Makridakis et al., 2020).”</i> (Makridakis et al., 2022).\n",
    "</b>\n",
    "<br> \n",
    "<br>\n",
    "References:\n",
    "<br>Makridakis, S., Spiliotis, E., Assimakopoulos, V. (2022). <i>M5 accuracy competition: Results, findings, and conclusions<i>, International Journal of Forecasting, Volume 38, Issue 4, 2022, Pages 1346-1364, ISSN 0169-2070, https://doi.org/10.1016/j.ijforecast.2021.11.013. (https://www.sciencedirect.com/science/article/pii/S0169207021001874 ) \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f8da1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
